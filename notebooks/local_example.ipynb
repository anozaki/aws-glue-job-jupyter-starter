{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Example\n",
    "\n",
    "This notebook plays around with spark using data found at https://github.com/astronexus/HYG-Database.\n",
    "For your convienence the data is copied in this repository but it is quite a large file.\n",
    "\n",
    "This setup assumes everything is located locally and all the data you are playing with is sitting inside your local development environment.\n",
    "This is a great way to get started on a large scale job by pulling a subset of data locally on to your machine to play with or to fix issue on a specific file that is causing you trouble.\n",
    "\n",
    "## System Args\n",
    "\n",
    "In a real glue executing, few parameters are passed in to your python script.\n",
    "Since we are running this in a local environment, we will fake these parameters.\n",
    "The following code will pass in the job name and the temp directory this script should use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv += ['--JOB_NAME', 'glue_script']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libs\n",
    "\n",
    "The first thing we do is to import the necessary library for our application.\n",
    "Since we installed Spark and Glue in the docker container, we are able to load the spark and glue modules here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import in all of the pyspark functions in\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType, IntegerType, StructField, StructType, DataType, DecimalType, DoubleType\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "# Import glue module components in\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from awsglue.dynamicframe import DynamicFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you executed the above inside the container, you should not get any errors and everything should work!\n",
    "\n",
    "## Configuration\n",
    "\n",
    "The first portion just specifies where the input directory is. This container puts your working directory in the root `/workspaces` folder. If you changed the default name of the checked out git repository, update the 2 line below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = \"file:///workspaces/aws-glue-job-jupyter-starter/data\"\n",
    "OUTPUT_PATH = \"file:///workspaces/aws-glue-job-jupyter-starter/output/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we load the command line argument and convert it to a format that Glue wants them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark/Glue Context\n",
    "\n",
    "Now create the Spark and Glue context.\n",
    "The following is pretty standard in most Glue example you will find.\n",
    "The only difference is the `getOrCreate()` method we are using.\n",
    "This method will allow us to rerun this block multiple times within this notebook.\n",
    "In a real Glue Job, you can simply create the context since you would only ever run it once.\n",
    "\n",
    "You will also notice the `AWS_REGION` is specified here as an environment variable.\n",
    "Since Glue library uses the AWS SDK, it will error if this environment variable is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: AWS_REGION=us-east-1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark.driver.port', '35925'),\n",
       " ('spark.executor.extraClassPath', '/opt/aws-glue-libs/jarsv1/*'),\n",
       " ('spark.app.id', 'local-1597587066157'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.driver.extraClassPath', '/opt/aws-glue-libs/jarsv1/*'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.driver.host', '4e108c19a2ce')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env AWS_REGION=us-east-1\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Structure\n",
    "\n",
    "Usually you wouldn't have this block of code.\n",
    "The most common method I use is to run a Glue Crawler and figure out most of this for us.\n",
    "Then use `ApplyMapping` to adjust any fields that are wrong.\n",
    "\n",
    "In this section we are configuring what the Glue Crawler would have given us and creating a DataFrame from these information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the schema structure\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),              # 1\n",
    "    StructField(\"hip\", IntegerType(), True),              # 2\n",
    "    StructField(\"hd\", IntegerType(), True),               # 3\n",
    "    StructField(\"hr\", IntegerType(), True),               # 4\n",
    "    StructField(\"gl\", StringType(), True),                # 5\n",
    "    StructField(\"bf\", StringType(), True),                # 6\n",
    "    StructField(\"proper\", StringType(), True),            # 7\n",
    "    StructField(\"ra\", DoubleType(), False),               # 8\n",
    "    StructField(\"dec\", DoubleType(), False),              # 9\n",
    "    StructField(\"dist\", DoubleType(), False),             # 10\n",
    "    StructField(\"pmra\", DoubleType(), False),             # 11\n",
    "    StructField(\"pmdec\", DoubleType(), False),            # 12\n",
    "    StructField(\"rv\", DoubleType(), False),               # 13\n",
    "    StructField(\"mag\", DoubleType(), False),              # 14\n",
    "    StructField(\"absmag\", DoubleType(), False),           # 15\n",
    "    StructField(\"spect\", StringType(), False),            # 16\n",
    "    StructField(\"ci\", DoubleType(), False),               # 17\n",
    "    StructField(\"x\", DoubleType(), False),                # 18\n",
    "    StructField(\"y\", DoubleType(), False),                # 19\n",
    "    StructField(\"z\", DoubleType(), False),                # 20\n",
    "    StructField(\"vx\", DoubleType(), False),               # 21\n",
    "    StructField(\"vy\", DoubleType(), False),               # 22\n",
    "    StructField(\"vz\", DoubleType(), False),               # 23\n",
    "    StructField(\"rarad\", DoubleType(), False),            # 24\n",
    "    StructField(\"decrad\", DoubleType(), False),           # 25\n",
    "    StructField(\"pmrarad\", DoubleType(), False),          # 26\n",
    "    StructField(\"pmdecrad\", DoubleType(), False),         # 27\n",
    "    StructField(\"bayer\", StringType(), True),             # 28\n",
    "    StructField(\"flam\", IntegerType(), True),             # 29\n",
    "    StructField(\"con\", StringType(), True),               # 30\n",
    "    StructField(\"comp\", IntegerType(), False),            # 31\n",
    "    StructField(\"comp_primary\", IntegerType(), True),     # 32\n",
    "    StructField(\"base\", StringType(), True),              # 33\n",
    "    StructField(\"lum\", DoubleType(), False),              # 34\n",
    "    StructField(\"var\", StringType(), True),               # 35\n",
    "    StructField(\"var_min\", DoubleType(), True),           # 36\n",
    "    StructField(\"var_max\", DoubleType(), True),           # 37\n",
    "])\n",
    "\n",
    "\n",
    "hygdata_v3_df = spark.read.format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option('quote', '\"') \\\n",
    "    .option(\"encoding\", \"UTF-8\") \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .option(\"columnNameOfCorruptRecord\", \"_corrupt_column\") \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(INPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DynamicFrame\n",
    " \n",
    "Finally we create a DynamicFrame from the Spark DataFrame ðŸŽ‰.\n",
    "You have a DynamicFrame running locally without spending those pretty pennies!\n",
    "All lines that comes after this would be more or less the same as what you would run inside your Glue Job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hygdata_v3_table = DynamicFrame.fromDF(hygdata_v3_df, glueContext, 'hygdata_v3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulate Data\n",
    "   \n",
    "As an example from the data we have loaded, lets way we want to only keep stars that are close to our sun's luminosity (means something close to 1.0).\n",
    "We can do this by using the DynamicFrame's filter method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 119614\n",
      "Filtered 705\n"
     ]
    }
   ],
   "source": [
    "print(\"Original {}\".format(hygdata_v3_table.toDF().count()))\n",
    "filtered_data = hygdata_v3_table.filter(f = lambda x: x['lum'] >= 0.99 and x['lum'] <= 1.1, transformation_ctx=\"filtered_data\")\n",
    "print(\"Filtered {}\".format(filtered_data.toDF().count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets add calculate the light years.\n",
    "The readme for the data mentios that anything above 100000 or negative is bad data.\n",
    "We filter out items that we don't care about first, then calcualte the light year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered bad data: 705\n"
     ]
    }
   ],
   "source": [
    "removed_bad_data = filtered_data.filter(f = lambda x: x['dist'] is not None and x['dist'] < 100000 and x['dist'] >=0, transformation_ctx=\"removed_bad_data\")\n",
    "print(\"Filtered bad data: {}\".format(removed_bad_data.toDF().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---------+-------+------------------+\n",
      "|    id|  hr|       bf|   dist|        light_year|\n",
      "+------+----+---------+-------+------------------+\n",
      "|     0|null|     null|    0.0|               0.0|\n",
      "| 15333|1010|  Zet2Ret|12.0322|        39.2490364|\n",
      "| 80095|6094|     null|12.7779|        41.6815098|\n",
      "| 79431|6060|18    Sco|13.9005|45.343430999999995|\n",
      "| 42319|3391| 3Pi 1UMa|14.3554|46.827314799999996|\n",
      "| 74952|5699|  Nu 2Lup|14.8126|        48.3187012|\n",
      "| 19028|1262|39    Tau|16.9377|        55.2507774|\n",
      "| 62014|4845|10    CVn|17.3762|        56.6811644|\n",
      "| 43602|3538|     null|17.3853|56.710848600000006|\n",
      "| 98643|7644|     null|17.7274|        57.8267788|\n",
      "| 98605|7683|     null|18.7899|        61.2926538|\n",
      "| 84783|6465|     null|19.5236|63.685983199999995|\n",
      "|  1799|  88| 9    Cet|20.8638| 68.05771560000001|\n",
      "|119398|null|     null|20.9205|         68.242671|\n",
      "|101710|7914|     null|20.9468|        68.3284616|\n",
      "|117998|null|     null|21.0482|        68.6592284|\n",
      "| 18366|null|     null|21.9829| 71.70821980000001|\n",
      "|  3490|null|     null|22.0556| 71.94536719999999|\n",
      "|103127|null|     null|22.1386|        72.2161132|\n",
      "|119271|7294|     null|22.9885| 74.98848699999999|\n",
      "+------+----+---------+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def add_light_year(d):\n",
    "    d['light_year'] = d['dist'] * 3.262\n",
    "    return d\n",
    "with_light_year = removed_bad_data.map(f=add_light_year, transformation_ctx=\"with_light_year\")\n",
    "with_light_year.select_fields([\"id\", \"hr\", \"bf\", \"dist\", \"light_year\"]).toDF().orderBy(\"light_year\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first entry is our sun so no surprise that it shows up at the top.\n",
    "Then we find our closes start that has a simlar light output as our sun (https://en.wikipedia.org/wiki/Zeta_Reticuli).\n",
    "\n",
    "## Saving\n",
    "\n",
    "Finally we want to save the file somewhere so we can use it.\n",
    "\n",
    "Since we are purely working in local environment, we simply use spark to save the data out.\n",
    "As far as I know, the DynamicFrame does not have a way to save to local disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_order_fixed = with_light_year.toDF().select(hygdata_v3_df.columns + ['light_year'])\n",
    "column_order_fixed.write.mode(\"overwrite\").csv(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
