{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use AWS Components\n",
    "\n",
    "Using all local component is by far the cheapest route, but some time you want to use things that are stored in AWS.\n",
    "In order to use the AWS components from this docker image, you'll need to make sure you setup the AWS credential properly.\n",
    "\n",
    "In this notebook, we will look at:\n",
    "\n",
    "* Access to AWS Glue Catalog\n",
    "* Access data in S3 bucket\n",
    "* Write modified data to S3 bucket\n",
    "\n",
    "## System Args\n",
    "\n",
    "Similar to full local setup, we will need to setup the fake parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv += ['--JOB_NAME', 'glue_script']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libs\n",
    "\n",
    "The first thing we do is to import the necessary library for our application. \n",
    "Since we installed Spark and Glue in the docker container, we are able to load the spark and glue modules here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import in all of the pyspark functions in\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType, IntegerType, StructField, StructType, DataType, DecimalType, DoubleType\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "# Import glue module components in\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from awsglue.dynamicframe import DynamicFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  AWS Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS Profile: ········\n",
      "AWS Region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "aws_profile = getpass.getpass(prompt=\"AWS Profile: \")\n",
    "aws_region = input(prompt=\"AWS Region: \")\n",
    "\n",
    "os.environ[\"AWS_PROFILE\"] = aws_profile\n",
    "os.environ[\"AWS_REGION\"] = aws_region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args\n",
    "\n",
    "Load the configuration args using the glue provided function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.executor.extraClassPath', '/opt/aws-glue-libs/jarsv1/*'),\n",
       " ('spark.app.id', 'local-1597640676340'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.driver.extraClassPath', '/opt/aws-glue-libs/jarsv1/*'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.host', '7b606bae576f'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.driver.port', '40465')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glue Catalog\n",
    "\n",
    "You will need to upload the hygdata_v3.csv to a S3 bucket and run glue crawler over the data.\n",
    "\n",
    "**TODO** some instruction on upload/running crawler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glue Catalog Database: etl_test\n",
      "Glue Catalog Table: etl_test_data\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- gl: string (nullable = true)\n",
      " |-- bf: string (nullable = true)\n",
      " |-- proper: string (nullable = true)\n",
      " |-- ra: double (nullable = true)\n",
      " |-- dec: double (nullable = true)\n",
      " |-- dist: double (nullable = true)\n",
      " |-- pmra: double (nullable = true)\n",
      " |-- pmdec: double (nullable = true)\n",
      " |-- rv: double (nullable = true)\n",
      " |-- mag: double (nullable = true)\n",
      " |-- absmag: double (nullable = true)\n",
      " |-- spect: string (nullable = true)\n",
      " |-- ci: double (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      " |-- vx: double (nullable = true)\n",
      " |-- vy: double (nullable = true)\n",
      " |-- vz: double (nullable = true)\n",
      " |-- rarad: double (nullable = true)\n",
      " |-- decrad: double (nullable = true)\n",
      " |-- pmrarad: double (nullable = true)\n",
      " |-- pmdecrad: double (nullable = true)\n",
      " |-- bayer: string (nullable = true)\n",
      " |-- con: string (nullable = true)\n",
      " |-- comp: long (nullable = true)\n",
      " |-- comp_primary: long (nullable = true)\n",
      " |-- base: string (nullable = true)\n",
      " |-- lum: double (nullable = true)\n",
      " |-- var: string (nullable = true)\n",
      " |-- hip: long (nullable = true)\n",
      " |-- hd: long (nullable = true)\n",
      " |-- var_min: double (nullable = true)\n",
      " |-- var_max: double (nullable = true)\n",
      " |-- hr: long (nullable = true)\n",
      " |-- flam: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "glue_database = input(\"Glue Catalog Database: \")\n",
    "glue_table = input(\"Glue Catalog Table: \")\n",
    "\n",
    "etl_test_data = glueContext.create_dynamic_frame.from_catalog(\n",
    "    database=glue_database, table_name=glue_table)\n",
    "etl_test_data.toDF().printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't dive too deep into what the crawler found, but in general it should be the same as what we specified manually.\n",
    "\n",
    "## Process Data\n",
    "\n",
    "We will process the data exactly the same as we did in the local setup.\n",
    "The output should be the same, since we are working with the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 119614\n",
      "Filtered 705\n"
     ]
    }
   ],
   "source": [
    "print(\"Original {}\".format(etl_test_data.toDF().count()))\n",
    "filtered_data = etl_test_data.filter(f = lambda x: x['lum'] >= 0.99 and x['lum'] <= 1.1, transformation_ctx=\"filtered_data\")\n",
    "print(\"Filtered {}\".format(filtered_data.toDF().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered bad data: 705\n"
     ]
    }
   ],
   "source": [
    "removed_bad_data = filtered_data.filter(f = lambda x: x['dist'] is not None and x['dist'] < 100000 and x['dist'] >=0, transformation_ctx=\"removed_bad_data\")\n",
    "print(\"Filtered bad data: {}\".format(removed_bad_data.toDF().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-------+------------------+----+\n",
      "|    id|       bf|   dist|        light_year|  hr|\n",
      "+------+---------+-------+------------------+----+\n",
      "|     0|         |    0.0|               0.0|null|\n",
      "| 15333|  Zet2Ret|12.0322|        39.2490364|1010|\n",
      "| 80095|         |12.7779|        41.6815098|6094|\n",
      "| 79431|18    Sco|13.9005|45.343430999999995|6060|\n",
      "| 42319| 3Pi 1UMa|14.3554|46.827314799999996|3391|\n",
      "| 74952|  Nu 2Lup|14.8126|        48.3187012|5699|\n",
      "| 19028|39    Tau|16.9377|        55.2507774|1262|\n",
      "| 62014|10    CVn|17.3762|        56.6811644|4845|\n",
      "| 43602|         |17.3853|56.710848600000006|3538|\n",
      "| 98643|         |17.7274|        57.8267788|7644|\n",
      "| 98605|         |18.7899|        61.2926538|7683|\n",
      "| 84783|         |19.5236|63.685983199999995|6465|\n",
      "|  1799| 9    Cet|20.8638| 68.05771560000001|  88|\n",
      "|119398|         |20.9205|         68.242671|null|\n",
      "|101710|         |20.9468|        68.3284616|7914|\n",
      "|117998|         |21.0482|        68.6592284|null|\n",
      "| 18366|         |21.9829| 71.70821980000001|null|\n",
      "|  3490|         |22.0556| 71.94536719999999|null|\n",
      "|103127|         |22.1386|        72.2161132|null|\n",
      "|119271|         |22.9885| 74.98848699999999|7294|\n",
      "+------+---------+-------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def add_light_year(d):\n",
    "    d['light_year'] = d['dist'] * 3.262\n",
    "    return d\n",
    "with_light_year = removed_bad_data.map(f=add_light_year, transformation_ctx=\"with_light_year\")\n",
    "with_light_year.select_fields([\"id\", \"hr\", \"bf\", \"dist\", \"light_year\"]).toDF().orderBy(\"light_year\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving\n",
    "\n",
    "Finally we want to save the file back to s3.\n",
    "Unlike our local setup, we will use glueContext to save the file back up to s3.\n",
    "\n",
    "you can easily use spark to save the file backup, but we are using glue here after all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket: ········\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<awsglue.dynamicframe.DynamicFrame at 0x7fb984b07588>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_bucket = getpass.getpass(\"Bucket: \")\n",
    "\n",
    "glueContext.write_dynamic_frame.from_options(\n",
    "    frame=with_light_year, \n",
    "    connection_type=\"s3\",\n",
    "    connection_options={\n",
    "        \"path\": \"s3://{}/processed/\".format(s3_bucket),\n",
    "    },\n",
    "    format=\"parquet\",\n",
    "    transformation_ctx=\"process_data\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can crawl over the newly created parquet file and query it from Athena!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
